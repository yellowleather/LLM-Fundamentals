{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yellowleather/LLM-Fundamentals/blob/main/1_Hands_on_with_Generative_AI_Demo_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hands-On with Generative AI\n",
        "\n",
        "This notebook demostrates how to generate text and images from input prompts. We show how to perform both tasks by calling APIs and with open models that we run locally in this notebook."
      ],
      "metadata": {
        "id": "nx0qiGWO7qWV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before getting into the examples we will install some packages that we will need."
      ],
      "metadata": {
        "id": "pVACWFAT7_k6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qp2JAR0-28Kt"
      },
      "outputs": [],
      "source": [
        "# Install packages that we will use\n",
        "!pip install openai==1.59.9\n",
        "!pip install accelerate==1.2.1\n",
        "!pip install diffusers==0.32.2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Use large language models to generate text"
      ],
      "metadata": {
        "id": "sHHQ0FfK3pZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import packages that you will use for accessing the OpenAI API\n",
        "import json\n",
        "from google.colab import drive\n",
        "from openai import OpenAI"
      ],
      "metadata": {
        "id": "qHqi4fZW3LL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will need an API key to access the [OpenAI API](https://openai.com/index/openai-api). We could load our API key from a file on Google drive. We do this to avoid hardcoding our personal API key in this notebook, which would make it visible to anyone who uses the notebook.\n",
        "\n",
        "Our API key could be in a file containing a JSON object of the form\n",
        "```\n",
        "{\n",
        "  \"api_key\": \"<MY_API_KEY>\"\n",
        "}\n",
        "```\n",
        "\n",
        "Or we could use Google Colab's built in secrets feature. We will use that instead."
      ],
      "metadata": {
        "id": "s_BOa3WVweTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For the Mount Google Drive option uncomment.\n",
        "# We will get our OpenAI API key from a file that we stored in Google Drive.\n",
        "# drive.mount(\"/content/gdrive\")\n",
        "# # Read in API key\n",
        "# with open(\"/content/gdrive/MyDrive/OpenAI/keys.json\", \"r\") as f:\n",
        "#   api_key = json.loads(f.read())[\"api_key\"]\n",
        "\n",
        "# for the other option\n",
        "import getpass\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "    if not userdata.get(\"OPENAI_API_KEY\"):\n",
        "        os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
        "    else:\n",
        "        os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "tiOTaZKz3j32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the OpenAI API to generate a response from an input prompt. To call the API, we will create an OpenAI client object. We can later use this same client object to generate images using the DALL-E model."
      ],
      "metadata": {
        "id": "oG5qnsSg9vRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an OpenAI client\n",
        "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))"
      ],
      "metadata": {
        "id": "9SJd5wNw3RJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now generate text from a given prompt using a single API call. Here we pass the prompt \"do snakes have ears?\", then use a GPT model to generate a response."
      ],
      "metadata": {
        "id": "gHLbKQ03-CmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[\n",
        "    {'role': 'system', 'content': 'do snakes have ears?'},\n",
        "    {'role': 'user', 'content': 'do snakes have ears?'},\n",
        "    {'role': 'assistant', 'content': 'No, snakes do not have external ears like mammals do. Instead, they have inner ears that are responsible for sensing vibrations and sound waves. This allows them to hear low-frequency sounds and detect movements in their environment.'},\n",
        "    {'role': 'user', 'content': 'XXXX'}\n",
        "]"
      ],
      "metadata": {
        "id": "Y0zGnssrQBwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'gpt-4o-mini'\n",
        "openai_response = client.chat.completions.create(\n",
        "    model = model_name,\n",
        "    messages = [\n",
        "         {'role': 'user', 'content': 'do snakes have ears?'}\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Print the response\n",
        "openai_response.choices[0].message.content"
      ],
      "metadata": {
        "id": "J3oPP_fX4VYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai_response.choices[0].message"
      ],
      "metadata": {
        "id": "8x8xqa-W4SSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Now we will download a model to this notebook and generate a response locally from this model. We will use the Hugging Face Transformers package to accomplish this. There are various open models we can use with Transformers. We will use Microsoft's Phi-3-mini model, which is a relatively small (3.8 billion parameters) but capable model."
      ],
      "metadata": {
        "id": "YLkPtSavLIQF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the Transformers package\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ],
      "metadata": {
        "id": "rSaf1UlXKVRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will need a tokenizer to convert our text input into a sequence of tokens and a model to generate a response from the provided context tokens."
      ],
      "metadata": {
        "id": "UPA_jXuh_UKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\",\n",
        "                                             device_map=\"cuda\",\n",
        "                                             torch_dtype=\"auto\",\n",
        "                                             trust_remote_code=True\n",
        "                                             )\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")"
      ],
      "metadata": {
        "id": "uOL7LdnKKVa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will evaluate the same prompt as above (\"do snakes have ears?\") with the Phi-3-mini model. We will first convert our prompt to tokens, generate the output tokens, then convert back to text to view the output."
      ],
      "metadata": {
        "id": "ffVL3iuw_h-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the input context\n",
        "messages = [{\"role\": \"user\", \"content\": \"do snakes have ears?\"}]\n",
        "inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "TWs8qAyc_Ry3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode(\"Generative\")"
      ],
      "metadata": {
        "id": "ofRvaIboGTFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(3251)"
      ],
      "metadata": {
        "id": "I2mD3Y-z_dWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs[0]"
      ],
      "metadata": {
        "id": "WM1HaO_-_-Zx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = model(inputs.to(\"cuda\"))"
      ],
      "metadata": {
        "id": "K9m3Q_tcEmLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass the context to the model and generate an output\n",
        "outputs = model.generate(inputs.to(\"cuda\"), max_new_tokens=128)"
      ],
      "metadata": {
        "id": "OtyyVYuyKVeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in outputs[0]:\n",
        "  print(token, tokenizer.decode(token))"
      ],
      "metadata": {
        "id": "vFpV7HeLAbpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = tokenizer.batch_decode(outputs)[0]\n",
        "text"
      ],
      "metadata": {
        "id": "8c7nZ5K4AbyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Use diffusion models to generate images"
      ],
      "metadata": {
        "id": "abhw2_ZiYt2v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now generate an image from given prompt using a single API call. We will use the same OpenAI API client that we opened before to call OpenAI's DALL-E model. Here we pass the prompt \"a realistic photograph of a snake with ears\", then use DALL-E 3 to generate an image."
      ],
      "metadata": {
        "id": "xajzH4zdAXQH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the OpenAI API to generate an Image from DALL-E\n",
        "response = client.images.generate(\n",
        "  model=\"dall-e-3\",\n",
        "  prompt=\"FRANKENSTEIN\",\n",
        "  size=\"1024x1024\",\n",
        "  quality=\"standard\",\n",
        "  n=1,\n",
        ")\n",
        "\n",
        "image_url = response.data[0].url"
      ],
      "metadata": {
        "id": "UbyPvylT4Vfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_url"
      ],
      "metadata": {
        "id": "NNY7BV2DUcCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The DALL-E API generates an image, then provides a URL that can be used to access the image. We pull the image data from this URL below, then display in the notebook."
      ],
      "metadata": {
        "id": "eE3YFKAHAq2t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import packages for displaying the generated image\n",
        "import urllib.request\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "\n",
        "# Load and display the image\n",
        "with urllib.request.urlopen(image_url) as url:\n",
        "    img = Image.open(BytesIO(url.read()))\n",
        "display(img.resize((500, 500)))\n",
        "#display(img)"
      ],
      "metadata": {
        "id": "tfv212Gb4VrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Now we will download a model to this notebook and generate an image locally from this model. We will use the Hugging Face Diffusers package to accomplish this. There are various open models we can use with Diffusers. We will use Stability AI's Stable Diffusion XL model for this.\n",
        "\n",
        "The free Colab T4 instance might run out of GPU memory if we run the code below after running the language model. You can restart the runtime before running the code below to free up the GPU memory."
      ],
      "metadata": {
        "id": "5UEoYLZDrybu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check on GPU memory usage\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "Cq8pkHnj02kY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the Diffusers package\n",
        "from diffusers import DiffusionPipeline\n",
        "import torch"
      ],
      "metadata": {
        "id": "6xgVyGO5rxTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we load the Stable Diffusion XL model into a pipeline that combines all of the components used to generate an image (e.g., a text encoder, a text-conditioned U-Net, a scheduler, and a variational autoencoder)."
      ],
      "metadata": {
        "id": "uR4TqYvXBcnz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Stable Diffusion XL model\n",
        "pipe = DiffusionPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "    torch_dtype=torch.float16,\n",
        "    use_safetensors=True,\n",
        "    variant=\"fp16\"\n",
        ")\n",
        "pipe.to(\"cuda\")"
      ],
      "metadata": {
        "id": "8RRa9Ju8zKy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that our model pipeline is loaded, we will pass the same prompt that we provided to DALL-E above (\"a realistic photograph of an astronaut riding a snake\") to Stable DIffusion XL."
      ],
      "metadata": {
        "id": "HUjcio4eBymh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate an image from a prompt\n",
        "prompt = \"a realistic photograph of an astronaut riding a snake\"\n",
        "image = pipe(prompt=prompt, width=512, height=512, num_inference_steps=50).images[0]\n",
        "image"
      ],
      "metadata": {
        "id": "8ZibGi-zrxa2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe"
      ],
      "metadata": {
        "id": "WGtkNiueX6R5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5UjhexhSms1y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}