{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yellowleather/LLM-Fundamentals/blob/main/agenteval_llm_judge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64a85daf",
      "metadata": {
        "id": "64a85daf"
      },
      "source": [
        "# ðŸ§ª LLM Judge Homework: Step-by-Step Evaluation\n",
        "In this assignment, you'll incrementally build an LLM-based judge to compare two summaries.\n",
        "\n",
        "You'll go through the following steps:\n",
        "1. Base judgment: which is better overall?\n",
        "2. Add rubric: accuracy, coverage, clarity\n",
        "3. Add explanations per rubric\n",
        "4. Add chain-of-thought reasoning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f55c5daf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f55c5daf",
        "outputId": "679647fb-4c63-417a-d262-99bb32e1c5f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.93.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.7.9)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "868d58bf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "868d58bf",
        "outputId": "183ddd92-20f1-446d-da19-c5dece81b5cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OpenAI API key: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        }
      ],
      "source": [
        "import openai\n",
        "from getpass import getpass\n",
        "openai.api_key = getpass('Enter your OpenAI API key: ')\n",
        "client = openai.OpenAI(api_key=openai.api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88d3c78d",
      "metadata": {
        "id": "88d3c78d"
      },
      "source": [
        "## ðŸ§¾ Example Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8a107eb",
      "metadata": {
        "id": "c8a107eb"
      },
      "outputs": [],
      "source": [
        "examples = [\n",
        "    {\n",
        "        \"id\": \"ex1\",\n",
        "        \"context\": \"The UN released a report warning of global temperature rise and called for urgent international action.\",\n",
        "        \"summary_a\": \"The UN warned that climate change is worsening and action is needed.\",\n",
        "        \"summary_b\": \"The UN praised global progress in reducing emissions.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"ex2\",\n",
        "        \"context\": \"NASA launched Artemis I, an uncrewed spacecraft that will orbit the Moon and return to Earth, preparing for human missions.\",\n",
        "        \"summary_a\": \"NASA launched Artemis I to prepare for future Moon missions.\",\n",
        "        \"summary_b\": \"NASA's Artemis I failed to launch due to engine problems.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"ex3\",\n",
        "        \"context\": \"A study found intermittent fasting improves blood sugar and cholesterol levels.\",\n",
        "        \"summary_a\": \"Intermittent fasting improves health markers like blood sugar and cholesterol.\",\n",
        "        \"summary_b\": \"Fasting was linked to poor nutrition and increased blood pressure.\"\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c71eb8e",
      "metadata": {
        "id": "8c71eb8e"
      },
      "source": [
        "## ðŸ”¹ Step 1: Base Judgment â€“ A or B?\n",
        "No rubric, just pick the better one and explain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8b653f3",
      "metadata": {
        "id": "b8b653f3"
      },
      "outputs": [],
      "source": [
        "def judge_base(context, summary_a, summary_b,client):\n",
        "    prompt = f'''\n",
        "You're evaluating two summaries of an article.\n",
        "\n",
        "Article:\n",
        "\\\"\\\"\\\"{context}\\\"\\\"\\\"\n",
        "\n",
        "Summary A:\n",
        "\\\"\\\"\\\"{summary_a}\\\"\\\"\\\"\n",
        "\n",
        "Summary B:\n",
        "\\\"\\\"\\\"{summary_b}\\\"\\\"\\\"\n",
        "\n",
        "Which one is better and why? Reply in JSON:\n",
        "{{\n",
        "  \"final_answer\": \"A\" or \"B\",\n",
        "  \"explanation\": \"...\"\n",
        "}}\n",
        "'''\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "    return response.choices[0].message.content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9741f99a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9741f99a",
        "outputId": "2305d70f-03b3-4568-9c9b-30cdf1c75296"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Step 1: Base Judgment\n",
            "```json\n",
            "{\n",
            "  \"final_answer\": \"A\",\n",
            "  \"explanation\": \"Summary A accurately reflects the main points of the article by highlighting the UN's warning about worsening climate change and the need for action. Summary B, on the other hand, inaccurately represents the article as it suggests that the UN is praising progress, which is not mentioned in the article.\"\n",
            "}\n",
            "```\n",
            "```json\n",
            "{\n",
            "  \"final_answer\": \"A\",\n",
            "  \"explanation\": \"Summary A accurately captures the main point of the article, which is that NASA successfully launched Artemis I to prepare for future Moon missions. Summary B is incorrect as it states that Artemis I failed to launch due to engine problems, which contradicts the information provided in the article.\"\n",
            "}\n",
            "```\n",
            "```json\n",
            "{\n",
            "  \"final_answer\": \"A\",\n",
            "  \"explanation\": \"Summary A accurately reflects the findings of the article by stating that intermittent fasting improves health markers such as blood sugar and cholesterol. Summary B, on the other hand, introduces information that contradicts the article by suggesting fasting is linked to poor nutrition and increased blood pressure, which is not supported by the original text.\"\n",
            "}\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "def run_judge_base(client):\n",
        "    print(\"Running Step 1: Base Judgment\")\n",
        "    for ex in examples:\n",
        "        result = judge_base(ex['context'], ex['summary_a'], ex['summary_b'],client)\n",
        "        print(result)\n",
        "\n",
        "run_judge_base(client)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "880f81b4",
      "metadata": {
        "id": "880f81b4"
      },
      "source": [
        "## ðŸ”¹ Step 2: Add Rubric Dimensions â€“ Accuracy, Coverage, Clarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "daea009c",
      "metadata": {
        "id": "daea009c"
      },
      "outputs": [],
      "source": [
        "def judge_with_rubric(context, summary_a, summary_b, client):\n",
        "    prompt = f'''\n",
        "Evaluate two summaries using this rubric (0â€“10 per dimension):\n",
        "- Accuracy\n",
        "- Coverage\n",
        "- Clarity\n",
        "\n",
        "Article:\n",
        "\\\"\\\"\\\"{context}\\\"\\\"\\\"\n",
        "\n",
        "Summary A:\n",
        "\\\"\\\"\\\"{summary_a}\\\"\\\"\\\"\n",
        "\n",
        "Summary B:\n",
        "\\\"\\\"\\\"{summary_b}\\\"\\\"\\\"\n",
        "\n",
        "Respond in JSON:\n",
        "{{\n",
        "  \"summary_a\": {{\"accuracy\": int, \"coverage\": int, \"clarity\": int}},\n",
        "  \"summary_b\": {{\"accuracy\": int, \"coverage\": int, \"clarity\": int}},\n",
        "  \"final_answer\": \"A\" or \"B\"\n",
        "}}\n",
        "'''\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "    return response.choices[0].message.content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3986709a",
      "metadata": {
        "id": "3986709a",
        "outputId": "78e36d59-eb38-423e-b2fa-66bb199f48a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Step 1: Base Judgment\n",
            "```json\n",
            "{\n",
            "  \"summary_a\": {\"accuracy\": 9, \"coverage\": 8, \"clarity\": 9},\n",
            "  \"summary_b\": {\"accuracy\": 2, \"coverage\": 2, \"clarity\": 8},\n",
            "  \"final_answer\": \"A\"\n",
            "}\n",
            "```\n",
            "```json\n",
            "{\n",
            "  \"summary_a\": {\"accuracy\": 10, \"coverage\": 6, \"clarity\": 9},\n",
            "  \"summary_b\": {\"accuracy\": 0, \"coverage\": 2, \"clarity\": 8},\n",
            "  \"final_answer\": \"A\"\n",
            "}\n",
            "```\n",
            "```json\n",
            "{\n",
            "  \"summary_a\": {\"accuracy\": 10, \"coverage\": 10, \"clarity\": 10},\n",
            "  \"summary_b\": {\"accuracy\": 0, \"coverage\": 0, \"clarity\": 5},\n",
            "  \"final_answer\": \"A\"\n",
            "}\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "def run_judge_base(client):\n",
        "    print(\"Running Step 1: Base Judgment\")\n",
        "    for ex in examples:\n",
        "        result = judge_with_rubric(ex['context'], ex['summary_a'], ex['summary_b'],client)\n",
        "        print(result)\n",
        "\n",
        "run_judge_base(client)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ea60c8b",
      "metadata": {
        "id": "8ea60c8b"
      },
      "source": [
        "## ðŸ”¹ Step 3: Add Explanations per Rubric Dimension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b7ff02c",
      "metadata": {
        "id": "6b7ff02c"
      },
      "outputs": [],
      "source": [
        "def judge_with_rubric_expl(context, summary_a, summary_b, client):\n",
        "    one_shot = '''\n",
        "Example:\n",
        "\n",
        "Article:\n",
        "\"\"\"The Mars rover Perseverance successfully collected its first rock samples, which will help scientists study signs of ancient life on Mars.\"\"\"\n",
        "\n",
        "Summary A:\n",
        "\"\"\"Perseverance gathered its first rock samples, advancing the mission to study past Martian life.\"\"\"\n",
        "\n",
        "Summary B:\n",
        "\"\"\"NASAâ€™s rover was unable to collect samples due to a drilling malfunction.\"\"\"\n",
        "\n",
        "Evaluation:\n",
        "{\n",
        "  \"summary_a\": {\n",
        "    \"accuracy\": 9,\n",
        "    \"accuracy_explanation\": \"Accurately reflects that the rover collected samples and the purpose of the mission.\",\n",
        "    \"coverage\": 9,\n",
        "    \"coverage_explanation\": \"Covers both the sampling and the scientific goal.\",\n",
        "    \"clarity\": 9,\n",
        "    \"clarity_explanation\": \"Clear and concise summary with no ambiguity.\"\n",
        "  },\n",
        "  \"summary_b\": {\n",
        "    \"accuracy\": 2,\n",
        "    \"accuracy_explanation\": \"Factually incorrect â€” the rover did collect samples successfully.\",\n",
        "    \"coverage\": 3,\n",
        "    \"coverage_explanation\": \"Misses all key points from the article and introduces incorrect information.\",\n",
        "    \"clarity\": 7,\n",
        "    \"clarity_explanation\": \"Somewhat clear, but misleading due to false claims.\"\n",
        "  },\n",
        "  \"final_answer\": \"A\"\n",
        "}\n",
        "\n",
        "---\n",
        "'''\n",
        "\n",
        "    prompt = f'''\n",
        "{one_shot}\n",
        "\n",
        "Now evaluate the following:\n",
        "\n",
        "Article:\n",
        "\\\"\\\"\\\"{context}\\\"\\\"\\\"\n",
        "\n",
        "Summary A:\n",
        "\\\"\\\"\\\"{summary_a}\\\"\\\"\\\"\n",
        "\n",
        "Summary B:\n",
        "\\\"\\\"\\\"{summary_b}\\\"\\\"\\\"\n",
        "\n",
        "}}\n",
        "'''\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[{\"role\": \"system\", \"content\": \"You are a strict JSON evaluator for summarization quality using rubric dimensions.\"},\n",
        "                  {\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f2f3e64",
      "metadata": {
        "id": "1f2f3e64",
        "outputId": "96d2afb9-e152-4896-c267-6409421748d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Step 3: Rubric + Explanation\n",
            "{\n",
            "  \"summary_a\": {\n",
            "    \"accuracy\": 8,\n",
            "    \"accuracy_explanation\": \"Generally aligns with the article as it mentions the UN's warning and need for action, but lacks specific details about the report or temperature rise.\",\n",
            "    \"coverage\": 7,\n",
            "    \"coverage_explanation\": \"Covers the key points about the UN's warning and the call for action, but does not mention the specific issue of temperature rise.\",\n",
            "    \"clarity\": 9,\n",
            "    \"clarity_explanation\": \"Clear and easy to understand, with a concise summary of the article's main points.\"\n",
            "  },\n",
            "  \"summary_b\": {\n",
            "    \"accuracy\": 2,\n",
            "    \"accuracy_explanation\": \"Factually incorrect since the article does not mention any praise for emission reduction, but rather a warning about temperature rise.\",\n",
            "    \"coverage\": 2,\n",
            "    \"coverage_explanation\": \"Fails to address the main content of the article, instead providing an incorrect statement.\",\n",
            "    \"clarity\": 8,\n",
            "    \"clarity_explanation\": \"While the sentence is clear in its language, it completely misrepresents the article's content.\"\n",
            "  },\n",
            "  \"final_answer\": \"A\"\n",
            "}\n",
            "{\n",
            "  \"summary_a\": {\n",
            "    \"accuracy\": 9,\n",
            "    \"accuracy_explanation\": \"Accurately reflects the successful launch of Artemis I and its purpose in preparing for future missions.\",\n",
            "    \"coverage\": 8,\n",
            "    \"coverage_explanation\": \"Mentions the launch and intent but lacks the detail about orbiting the Moon and returning to Earth.\",\n",
            "    \"clarity\": 9,\n",
            "    \"clarity_explanation\": \"Clear and concise, providing the main purpose without any ambiguity.\"\n",
            "  },\n",
            "  \"summary_b\": {\n",
            "    \"accuracy\": 2,\n",
            "    \"accuracy_explanation\": \"Factually incorrect â€” the article specifies that Artemis I launched successfully, not that it failed.\",\n",
            "    \"coverage\": 3,\n",
            "    \"coverage_explanation\": \"Misrepresents the article's main point and omits all accurate details, adding false information instead.\",\n",
            "    \"clarity\": 7,\n",
            "    \"clarity_explanation\": \"Somewhat clear in language, but misleading due to inaccurate content.\"\n",
            "  },\n",
            "  \"final_answer\": \"A\"\n",
            "}\n",
            "{\n",
            "  \"summary_a\": {\n",
            "    \"accuracy\": 9,\n",
            "    \"accuracy_explanation\": \"The summary accurately reflects the findings of the study, specifically focusing on improvements in blood sugar and cholesterol levels.\",\n",
            "    \"coverage\": 8,\n",
            "    \"coverage_explanation\": \"Covers the primary findings of the article, mentioning both health markers (blood sugar and cholesterol) discussed in the study.\",\n",
            "    \"clarity\": 9,\n",
            "    \"clarity_explanation\": \"The summary is clear, concise, and directly conveys the positive health outcomes of intermittent fasting.\"\n",
            "  },\n",
            "  \"summary_b\": {\n",
            "    \"accuracy\": 2,\n",
            "    \"accuracy_explanation\": \"Factually incorrect â€” the study indicates improvements, not poorer nutrition or increased blood pressure.\",\n",
            "    \"coverage\": 2,\n",
            "    \"coverage_explanation\": \"Fails to address the key findings of the article and introduces incorrect information not present in the text.\",\n",
            "    \"clarity\": 7,\n",
            "    \"clarity_explanation\": \"While the statement is clear, it misleads by providing inaccurate information not supported by the article.\"\n",
            "  },\n",
            "  \"final_answer\": \"A\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "def run_judge_rubric_expl(client):\n",
        "    print(\"Running Step 3: Rubric + Explanation\")\n",
        "    for ex in examples:\n",
        "        result = judge_with_rubric_expl(ex['context'], ex['summary_a'], ex['summary_b'],client)\n",
        "        print(result)\n",
        "\n",
        "run_judge_rubric_expl(client)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6ee5a25",
      "metadata": {
        "id": "b6ee5a25"
      },
      "source": [
        "## ðŸ”¹ Step 4: Add Chain-of-Thought Reasoning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "210ad1b6",
      "metadata": {
        "id": "210ad1b6"
      },
      "outputs": [],
      "source": [
        "def judge_chain_of_thought(context, summary_a, summary_b, client):\n",
        "    prompt = f\"\"\"\n",
        "\n",
        "Now evaluate:\n",
        "\n",
        "Article:\n",
        "\\\"\\\"\\\"{context}\\\"\\\"\\\"\n",
        "\n",
        "Summary A:\n",
        "\\\"\\\"\\\"{summary_a}\\\"\\\"\\\"\n",
        "\n",
        "Summary B:\n",
        "\\\"\\\"\\\"{summary_b}\\\"\\\"\\\"\n",
        "\n",
        "Reason step-by-step through **each dimension** before assigning scores.\n",
        "\n",
        "Respond only in JSON:\n",
        "{{\n",
        "  \"reasoning\": \"...\",\n",
        "  \"summary_a\": {{\"accuracy\": int, \"coverage\": int, \"clarity\": int}},\n",
        "  \"summary_b\": {{\"accuracy\": int, \"coverage\": int, \"clarity\": int}},\n",
        "  \"final_answer\": \"A\" or \"B\"\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a rubric-based evaluator that reasons step-by-step before scoring.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e871f848",
      "metadata": {
        "id": "e871f848",
        "outputId": "5c888662-a939-40bd-e3f5-d228d04ee53e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Step 4: Chain-of-Thought Judgment\n",
            "```json\n",
            "{\n",
            "  \"reasoning\": \"The evaluation will be conducted across three dimensions: accuracy, coverage, and clarity.\\n\\n- **Accuracy**: This dimension assesses whether the summaries correctly reflect the key messages of the original article.\\n  - Summary A accurately captures the essence of the article by emphasizing both the worsening state of climate change and the need for action. Thus, it merits a high score for accuracy.\\n  - Summary B is inaccurate because it misrepresents the article's message by mentioning that the UN praised progress in reducing emissions, which is not mentioned in the article. This results in a low score for accuracy.\\n\\n- **Coverage**: This dimension evaluates whether the summaries include the essential points from the article.\\n  - Summary A covers both aspects mentioned in the article: the warning about temperature rise and the call for urgent action, giving it full marks for coverage.\\n  - Summary B fails to cover the core points of the article, focusing instead on an incorrect assertion about emissions, resulting in a low score for coverage.\\n\\n- **Clarity**: This dimension judges whether the summaries are clear and concise.\\n  - Summary A is concise and clearly conveys the main message, deserving a high score for clarity.\\n  - Summary B, while clear, is misleading in content which affects the perception of clarity. However, it is not verbose or complicated, so it might still receive a moderate score for clarity.\\n\\nGiven this reasoning, Summary A is the more accurate and representative summary of the article.\",\n",
            "  \"summary_a\": {\"accuracy\": 5, \"coverage\": 5, \"clarity\": 5},\n",
            "  \"summary_b\": {\"accuracy\": 1, \"coverage\": 1, \"clarity\": 3},\n",
            "  \"final_answer\": \"A\"\n",
            "}\n",
            "```\n",
            "{\n",
            "  \"reasoning\": \"To evaluate summaries, I will consider the dimensions of accuracy, coverage, and clarity. \\n\\n1. **Accuracy:** This measures whether the information in the summary truly reflects the details in the source article.\\n   - **Summary A**: It accurately states that NASA launched Artemis I and that it's for preparing future missions, which aligns with the article.\\n   - **Summary B**: The claim that Artemis I failed to launch due to engine problems is incorrect according to the article.\\n\\n2. **Coverage:** This evaluates whether the summary captures significant details from the article.\\n   - **Summary A**: It covers the key point that Artemis I was launched to prepare for future missions, but it doesnâ€™t mention that Artemis I is an uncrewed spacecraft that will orbit the Moon and return to Earth.\\n   - **Summary B**: It does not capture any of the main points from the article since it presents false information.\\n\\n3. **Clarity:** This looks at how easy the summary is to understand.\\n   - **Summary A**: The language is straightforward and conveys the key purpose of the mission effectively.\\n   - **Summary B**: Despite being clear, itâ€™s misleading and incorrect, which affects overall clarity of the intended message.\\n\\nConsidering all aspects and how well they reflect the source, Summary A is vastly superior.\",\n",
            "  \"summary_a\": {\n",
            "    \"accuracy\": 4,\n",
            "    \"coverage\": 3,\n",
            "    \"clarity\": 5\n",
            "  },\n",
            "  \"summary_b\": {\n",
            "    \"accuracy\": 1,\n",
            "    \"coverage\": 1,\n",
            "    \"clarity\": 2\n",
            "  },\n",
            "  \"final_answer\": \"A\"\n",
            "}\n",
            "{\n",
            "  \"reasoning\": \"First, evaluating accuracy, Summary A states that intermittent fasting improves blood sugar and cholesterol, aligning accurately with the article's findings. In contrast, Summary B incorrectly links fasting to poor nutrition and increased blood pressure, which is not supported by the article. For coverage, Summary A mentions 'health markers like blood sugar and cholesterol,' capturing the essentials of the article. Summary B fails to cover the article's primary points by instead introducing inaccuracies. In clarity, both summaries are clear and concise, but given Summary A's alignment with the article, it provides a clearer reflection of the main findings, whereas Summary B is misleading due to incorrect information.\",\n",
            "  \"summary_a\": {\"accuracy\": 5, \"coverage\": 5, \"clarity\": 5},\n",
            "  \"summary_b\": {\"accuracy\": 1, \"coverage\": 1, \"clarity\": 4},\n",
            "  \"final_answer\": \"A\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "def run_judge_chain_of_thought(client):\n",
        "    print(\"Running Step 4: Chain-of-Thought Judgment\")\n",
        "    for ex in examples:\n",
        "        result = judge_chain_of_thought(ex['context'], ex['summary_a'], ex['summary_b'],client)\n",
        "        print(result)\n",
        "\n",
        "run_judge_chain_of_thought(client)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "688a7626",
      "metadata": {
        "id": "688a7626"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "vscode",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}